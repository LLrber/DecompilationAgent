"""
MCPå·¥å…·å‡½æ•°å®ç°

åŒ…å«æ‰€æœ‰ç”¨äºä»£ç åˆ†æçš„MCPå·¥å…·å‡½æ•°ã€‚
"""

import re
import os
from typing import Dict, List, Any, Tuple
from dataclasses import asdict
from collections import defaultdict

from ..core.analyzer import CodeAnalyzer
from ..models.data_models import FunctionInfo

# åˆ›å»ºå…¨å±€åˆ†æå™¨å®ä¾‹
analyzer = CodeAnalyzer()

# å…¨å±€åˆ†æå†å²è®°å½•ï¼ˆç”¨äºæ™ºèƒ½æ¨èï¼‰
analysis_history = []

def add_to_history(tool_name: str, result: Dict[str, Any]):
    """æ·»åŠ åˆ†æå†å²è®°å½•"""
    analysis_history.append({
        "tool": tool_name,
        "timestamp": __import__('time').time(),
        "result": result
    })
    # ä¿æŒæœ€è¿‘50æ¡è®°å½•
    if len(analysis_history) > 50:
        analysis_history.pop(0)

def get_smart_recommendations(current_tool: str, current_result: Dict[str, Any]) -> Dict[str, Any]:
    """ç”Ÿæˆæ™ºèƒ½æ¨è"""
    recommendations = []
    suggested_inputs = []
    
    if current_tool == "chunk_code_tool":
        if current_result.get("total_functions", 0) > 0:
            recommendations.extend([
                ("analyze_dependencies_tool", "åˆ†æå‡½æ•°é—´çš„è°ƒç”¨å…³ç³»å’Œä¾èµ–"),
                ("search_code_tool", "æœç´¢ç‰¹å®šçš„å‡½æ•°æˆ–å…³é”®è¯"),
                ("security_audit_tool", "æ£€æµ‹æ½œåœ¨çš„å®‰å…¨é—®é¢˜")
            ])
            suggested_inputs.extend([
                "åˆ†æä¾èµ–å…³ç³»",
                "æœç´¢ modbus",
                "å®‰å…¨å®¡è®¡",
                f"åˆ†æå‡½æ•° {current_result.get('chunks', [{}])[0].get('functions', [{}])[0].get('name', 'å‡½æ•°å')}"
            ])
    
    elif current_tool == "analyze_function_tool":
        func_name = current_result.get("function_name", "")
        if current_result.get("security_concerns"):
            recommendations.append(("security_audit_tool", "æ·±å…¥åˆ†æå®‰å…¨é—®é¢˜"))
        if current_result.get("complexity_indicators", {}).get("line_count", 0) > 50:
            recommendations.append(("refactor_function_tool", "è·å–é‡æ„å»ºè®®"))
        recommendations.extend([
            ("search_code_tool", f"æœç´¢è°ƒç”¨ {func_name} çš„å…¶ä»–ä½ç½®"),
            ("analyze_data_structures_tool", "åˆ†æç›¸å…³çš„æ•°æ®ç»“æ„")
        ])
        suggested_inputs.extend([
            f"æœç´¢ {func_name}",
            "åˆ†ææ•°æ®ç»“æ„",
            "é‡æ„å»ºè®®"
        ])
    
    elif current_tool == "analyze_dependencies_tool":
        critical_funcs = [node["name"] for node in current_result.get("dependency_nodes", []) 
                         if node.get("importance_score", 0) > 7]
        if critical_funcs:
            recommendations.append(("analyze_function_tool", f"æ·±å…¥åˆ†æå…³é”®å‡½æ•°: {', '.join(critical_funcs[:3])}"))
            for func in critical_funcs[:3]:
                suggested_inputs.append(f"åˆ†æå‡½æ•° {func}")
        recommendations.extend([
            ("generate_mermaid_chart_tool", "ç”Ÿæˆå¯è§†åŒ–ä¾èµ–å›¾"),
            ("security_audit_tool", "æ£€æµ‹æ•´ä½“å®‰å…¨é—®é¢˜")
        ])
        suggested_inputs.extend([
            "ç”Ÿæˆä¾èµ–å›¾",
            "å®‰å…¨å®¡è®¡"
        ])
    
    elif current_tool == "search_code_tool":
        results = current_result.get("results", [])
        if results:
            func_names = [r.get("function_name") for r in results if r.get("function_name")]
            if func_names:
                recommendations.append(("analyze_function_tool", f"åˆ†ææœç´¢åˆ°çš„å‡½æ•°"))
                for func in func_names[:3]:
                    suggested_inputs.append(f"åˆ†æå‡½æ•° {func}")
        recommendations.extend([
            ("analyze_data_structures_tool", "åˆ†æç›¸å…³æ•°æ®ç»“æ„"),
            ("security_audit_tool", "å®‰å…¨å®¡è®¡")
        ])
    
    elif current_tool == "security_audit_tool":
        issues = current_result.get("security_issues", [])
        if issues:
            recommendations.extend([
                ("refactor_function_tool", "è·å–ä¿®å¤å»ºè®®"),
                ("search_code_tool", "æœç´¢ç›¸ä¼¼çš„å®‰å…¨é—®é¢˜")
            ])
            suggested_inputs.extend([
                "é‡æ„å»ºè®®",
                "æœç´¢ strcpy",
                "æœç´¢ malloc"
            ])
    
    elif current_tool == "analyze_data_structures_tool":
        structs = current_result.get("structures", [])
        if structs:
            recommendations.extend([
                ("search_code_tool", "æœç´¢ç»“æ„ä½“çš„ä½¿ç”¨ä½ç½®"),
                ("security_audit_tool", "æ£€æŸ¥ç»“æ„ä½“ç›¸å…³çš„å®‰å…¨é—®é¢˜")
            ])
            for struct in structs[:3]:
                suggested_inputs.append(f"æœç´¢ {struct.get('name', '')}")
    
    # é€šç”¨æ¨è
    recommendations.append(("smart_assistant_tool", "è·å–ä¸ªæ€§åŒ–åˆ†æå»ºè®®"))
    suggested_inputs.append("æ™ºèƒ½åŠ©æ‰‹")
    
    return {
        "recommendations": recommendations,
        "suggested_inputs": suggested_inputs,
        "analysis_tips": [
            "ğŸ’¡ ä½¿ç”¨'æœç´¢'å‘½ä»¤å¿«é€Ÿå®šä½å…³é”®ä»£ç ",
            "ğŸ” ä½¿ç”¨'åˆ†æå‡½æ•°'æ·±å…¥ç†è§£å…·ä½“å®ç°",
            "ğŸ›¡ï¸ ä½¿ç”¨'å®‰å…¨å®¡è®¡'å‘ç°æ½œåœ¨é—®é¢˜",
            "ğŸ“Š ä½¿ç”¨'ç”Ÿæˆä¾èµ–å›¾'å¯è§†åŒ–ä»£ç ç»“æ„"
        ]
    }


def chunk_code(file_path: str, max_chunk_size: int = 2000) -> Dict[str, Any]:
    """
    å°†é•¿ä»£ç æ–‡ä»¶åˆ†å—å¤„ç†ï¼Œè§£å†³å¤§æ¨¡å‹ä¸Šä¸‹æ–‡é™åˆ¶
    
    Args:
        file_path: ä»£ç æ–‡ä»¶è·¯å¾„
        max_chunk_size: æœ€å¤§å—å¤§å°ï¼ˆè¡Œæ•°ï¼‰
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        return {"error": f"æ— æ³•è¯»å–æ–‡ä»¶: {str(e)}"}
    
    functions = analyzer.extract_functions(content)
    
    chunks = []
    current_chunk = []
    current_size = 0
    
    for func in functions:
        func_size = func.end_line - func.start_line
        
        if current_size + func_size > max_chunk_size and current_chunk:
            chunks.append({
                "id": f"chunk_{len(chunks)}",
                "functions": [asdict(f) for f in current_chunk],
                "size": current_size
            })
            current_chunk = [func]
            current_size = func_size
        else:
            current_chunk.append(func)
            current_size += func_size
    
    if current_chunk:
        chunks.append({
            "id": f"chunk_{len(chunks)}",
            "functions": [asdict(f) for f in current_chunk],
            "size": current_size
        })
    
    result = {
        "total_chunks": len(chunks),
        "chunks": chunks,
        "file_info": {
            "path": file_path,
            "total_lines": len(content.split('\n')),
            "total_functions": len(functions)
        }
    }
    
    # æ·»åŠ åˆ°å†å²è®°å½•å¹¶ç”Ÿæˆæ¨è
    add_to_history("chunk_code_tool", result)
    result["smart_recommendations"] = get_smart_recommendations("chunk_code_tool", result)
    
    return result


def analyze_function(function_code: str, function_name: str) -> Dict[str, Any]:
    """
    æ·±åº¦åˆ†æå•ä¸ªå‡½æ•°
    
    Args:
        function_code: å‡½æ•°ä»£ç 
        function_name: å‡½æ•°å
    """
    
    lines = function_code.split('\n')
    complexity_indicators = {
        'loops': len(re.findall(r'\b(for|while|do)\b', function_code)),
        'conditions': len(re.findall(r'\b(if|else|switch)\b', function_code)),
        'function_calls': len(re.findall(r'\w+\s*\(', function_code)),
        'line_count': len([l for l in lines if l.strip()]),
        'nested_blocks': function_code.count('{')
    }
    
    # ç”¨é€”æ¨æ–­
    purpose_hints = []
    name_lower = function_name.lower()
    
    if any(word in name_lower for word in ['init', 'initialize', 'setup', 'create']):
        purpose_hints.append("åˆå§‹åŒ–å‡½æ•°")
    if any(word in name_lower for word in ['free', 'destroy', 'cleanup', 'close']):
        purpose_hints.append("èµ„æºé‡Šæ”¾å‡½æ•°")
    if any(word in name_lower for word in ['get', 'read', 'fetch']):
        purpose_hints.append("æ•°æ®è·å–å‡½æ•°")
    if any(word in name_lower for word in ['set', 'write', 'store']):
        purpose_hints.append("æ•°æ®è®¾ç½®å‡½æ•°")
    if any(word in name_lower for word in ['send', 'recv', 'transmit']):
        purpose_hints.append("ç½‘ç»œé€šä¿¡å‡½æ•°")
    if 'modbus' in name_lower:
        purpose_hints.append("Modbusåè®®å‡½æ•°")
    if 'tcp' in name_lower:
        purpose_hints.append("TCPç½‘ç»œå‡½æ•°")
    if 'rtu' in name_lower:
        purpose_hints.append("RTUä¸²å£å‡½æ•°")
    
    # é”™è¯¯å¤„ç†åˆ†æ
    error_handling = []
    if 'return -1' in function_code or 'return NULL' in function_code:
        error_handling.append("åŒ…å«é”™è¯¯è¿”å›å€¼")
    if 'errno' in function_code:
        error_handling.append("ä½¿ç”¨errnoé”™è¯¯å¤„ç†")
    if re.search(r'\bif\s*\([^)]*==\s*NULL\)', function_code):
        error_handling.append("åŒ…å«NULLæ£€æŸ¥")
    
    # å®‰å…¨é£é™©åˆ†æ
    security_concerns = []
    if re.search(r'\b(strcpy|strcat|sprintf|gets)\b', function_code):
        security_concerns.append("ä½¿ç”¨ä¸å®‰å…¨çš„å­—ç¬¦ä¸²å‡½æ•°")
    if re.search(r'\bmalloc\b', function_code) and 'free' not in function_code:
        security_concerns.append("æ½œåœ¨å†…å­˜æ³„æ¼")
    
    result = {
        "function_name": function_name,
        "complexity_indicators": complexity_indicators,
        "purpose_hints": purpose_hints,
        "error_handling": error_handling,
        "security_concerns": security_concerns,
        "analysis": {
            "estimated_purpose": " | ".join(purpose_hints) if purpose_hints else "é€šç”¨å¤„ç†å‡½æ•°",
            "complexity_level": (
                "æé«˜" if complexity_indicators['line_count'] > 100 else
                "é«˜" if complexity_indicators['line_count'] > 50 else
                "ä¸­" if complexity_indicators['line_count'] > 20 else
                "ä½"
            ),
            "reliability": (
                "ä¼˜ç§€" if len(error_handling) >= 3 else
                "è‰¯å¥½" if len(error_handling) >= 2 else
                "ä¸€èˆ¬" if len(error_handling) >= 1 else
                "éœ€è¦æ”¹è¿›"
            ),
            "security_risk": (
                "é«˜" if len(security_concerns) >= 2 else
                "ä¸­" if len(security_concerns) == 1 else
                "ä½"
            )
        }
    }
    
    # æ·»åŠ åˆ°å†å²è®°å½•å¹¶ç”Ÿæˆæ¨è
    add_to_history("analyze_function_tool", result)
    result["smart_recommendations"] = get_smart_recommendations("analyze_function_tool", result)
    
    return result


def analyze_dependencies(chunks_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    åˆ†æå‡½æ•°é—´ä¾èµ–å…³ç³»
    
    Args:
        chunks_data: ä»chunk_codeè¿”å›çš„æ•°æ®
    """
    
    if "error" in chunks_data:
        return {"error": "æ— æ•ˆçš„åˆ†å—æ•°æ®"}
    
    # æ”¶é›†æ‰€æœ‰å‡½æ•°å’Œè°ƒç”¨å…³ç³»
    all_functions = {}
    call_graph = []
    
    for chunk in chunks_data.get("chunks", []):
        for func_data in chunk.get("functions", []):
            func_name = func_data["name"]
            calls = func_data.get("calls", [])
            
            all_functions[func_name] = {
                "calls": calls,
                "called_by": [],
                "complexity": func_data.get("complexity", 1),
                "line_count": func_data.get("end_line", 0) - func_data.get("start_line", 0)
            }
            
            for called_func in calls:
                call_graph.append((func_name, called_func))
    
    # æ„å»ºåå‘è°ƒç”¨å…³ç³»
    for caller, callee in call_graph:
        if callee in all_functions:
            all_functions[callee]["called_by"].append(caller)
    
    # è®¡ç®—é‡è¦æ€§è¯„åˆ†
    dependency_nodes = []
    for func_name, func_info in all_functions.items():
        calls_in = len(func_info["called_by"])
        calls_out = len(func_info["calls"])
        complexity = func_info["complexity"]
        
        importance_score = (calls_in * 2) + (calls_out * 0.5) + (complexity * 0.1)
        
        # æ¨¡å—åˆ†ç»„
        module_group = "core"
        if "modbus" in func_name.lower():
            module_group = "modbus"
        elif "tcp" in func_name.lower():
            module_group = "network"
        elif "rtu" in func_name.lower():
            module_group = "serial"
        elif any(word in func_name.lower() for word in ["init", "free"]):
            module_group = "lifecycle"
        elif any(word in func_name.lower() for word in ["get", "set"]):
            module_group = "data_access"
        
        dependency_nodes.append({
            "name": func_name,
            "calls": func_info["calls"],
            "called_by": func_info["called_by"],
            "importance_score": importance_score,
            "module_group": module_group,
            "complexity": complexity
        })
    
    # æŒ‰é‡è¦æ€§æ’åº
    dependency_nodes.sort(key=lambda x: x["importance_score"], reverse=True)
    
    # è¯†åˆ«å…³é”®å‡½æ•°
    critical_count = max(1, len(dependency_nodes) // 10)
    critical_functions = dependency_nodes[:critical_count]
    
    # æ¨¡å—åˆ†ç»„
    modules = {}
    for node in dependency_nodes:
        module = node["module_group"]
        if module not in modules:
            modules[module] = []
        modules[module].append(node["name"])
    
    result = {
        "total_functions": len(all_functions),
        "total_relationships": len(call_graph),
        "dependency_nodes": dependency_nodes,
        "critical_functions": [f["name"] for f in critical_functions],
        "modules": modules,
        "call_graph": call_graph,
        "statistics": {
            "max_calls_in": max([len(f["called_by"]) for f in all_functions.values()], default=0),
            "max_calls_out": max([len(f["calls"]) for f in all_functions.values()], default=0),
            "avg_complexity": sum([f["complexity"] for f in all_functions.values()]) / len(all_functions) if all_functions else 0
        }
    }
    
    # æ·»åŠ åˆ°å†å²è®°å½•å¹¶ç”Ÿæˆæ¨è
    add_to_history("analyze_dependencies_tool", result)
    result["smart_recommendations"] = get_smart_recommendations("analyze_dependencies_tool", result)
    
    return result


def generate_mermaid_chart(dependency_data: Dict[str, Any], chart_type: str = "flowchart", max_nodes: int = 20) -> str:
    """
    ç”ŸæˆMermaidå›¾è¡¨
    
    Args:
        dependency_data: ä¾èµ–åˆ†ææ•°æ®
        chart_type: å›¾è¡¨ç±»å‹ ("flowchart" | "graph" | "mindmap")
        max_nodes: æœ€å¤§èŠ‚ç‚¹æ•°
    """
    
    if "error" in dependency_data:
        return "é”™è¯¯: æ— æ•ˆçš„ä¾èµ–æ•°æ®"
    
    call_graph = dependency_data.get("call_graph", [])
    modules = dependency_data.get("modules", {})
    
    # é™åˆ¶èŠ‚ç‚¹æ•°
    call_graph = call_graph[:max_nodes]
    
    if chart_type == "flowchart":
        mermaid_code = ["flowchart TD"]
        
        added_nodes = set()
        for caller, callee in call_graph:
            clean_caller = re.sub(r'[^a-zA-Z0-9_]', '_', caller)
            clean_callee = re.sub(r'[^a-zA-Z0-9_]', '_', callee)
            
            if caller not in added_nodes:
                mermaid_code.append(f'    {clean_caller}["{caller}"]')
                added_nodes.add(caller)
            
            if callee not in added_nodes:
                mermaid_code.append(f'    {clean_callee}["{callee}"]')
                added_nodes.add(callee)
            
            mermaid_code.append(f'    {clean_caller} --> {clean_callee}')
    
    elif chart_type == "mindmap":
        mermaid_code = ["mindmap"]
        mermaid_code.append("  root((ä»£ç ç»“æ„))")
        
        for module, functions in modules.items():
            if functions:
                mermaid_code.append(f"    {module}")
                for func in functions[:5]:  # æ¯ä¸ªæ¨¡å—æœ€å¤š5ä¸ªå‡½æ•°
                    clean_func = re.sub(r'[^a-zA-Z0-9_]', '_', func)
                    mermaid_code.append(f"      {clean_func}")
    
    else:  # graph
        mermaid_code = ["graph LR"]
        for caller, callee in call_graph:
            clean_caller = re.sub(r'[^a-zA-Z0-9_]', '_', caller)
            clean_callee = re.sub(r'[^a-zA-Z0-9_]', '_', callee)
            mermaid_code.append(f'    {clean_caller}[{caller}] --> {clean_callee}[{callee}]')
    
    return '\n'.join(mermaid_code)


def refactor_function(function_code: str, function_name: str) -> Dict[str, Any]:
    """
    æä¾›å‡½æ•°é‡æ„å»ºè®®
    
    Args:
        function_code: å‡½æ•°ä»£ç 
        function_name: å‡½æ•°å
    """
    
    suggestions = []
    
    # æ£€æŸ¥å‡½æ•°é•¿åº¦
    lines = function_code.split('\n')
    if len(lines) > 50:
        suggestions.append("å‡½æ•°è¿‡é•¿ï¼Œå»ºè®®æ‹†åˆ†ä¸ºæ›´å°çš„å‡½æ•°")
    
    # æ£€æŸ¥å¤æ‚åº¦
    complexity = analyzer._calculate_complexity(function_code)
    if complexity > 15:
        suggestions.append("å‡½æ•°å¤æ‚åº¦è¿‡é«˜ï¼Œå»ºè®®ç®€åŒ–é€»è¾‘")
    
    # æ£€æŸ¥å˜é‡å‘½å
    single_vars = re.findall(r'\b[a-z]\b', function_code)
    if single_vars:
        suggestions.append(f"å»ºè®®ä½¿ç”¨æ›´å…·æè¿°æ€§çš„å˜é‡åæ›¿æ¢: {', '.join(set(single_vars))}")
    
    # æ£€æŸ¥é”™è¯¯å¤„ç†
    if 'return -1' not in function_code and 'return NULL' not in function_code:
        suggestions.append("å»ºè®®æ·»åŠ é”™è¯¯å¤„ç†å’Œè¿”å›å€¼æ£€æŸ¥")
    
    # æ£€æŸ¥é­”æ•°
    magic_numbers = re.findall(r'\b\d{2,}\b', function_code)
    if magic_numbers:
        suggestions.append(f"å»ºè®®ä¸ºé­”æ•°å®šä¹‰å¸¸é‡: {', '.join(set(magic_numbers))}")
    
    # ç”Ÿæˆæ”¹è¿›è¯„åˆ†
    improvement_score = max(0, 100 - len(suggestions) * 15)
    
    result = {
        "function_name": function_name,
        "suggestions": suggestions,
        "improvement_score": improvement_score,
        "priority": "é«˜" if len(suggestions) > 3 else "ä¸­" if len(suggestions) > 1 else "ä½"
    }
    
    # æ·»åŠ åˆ°å†å²è®°å½•å¹¶ç”Ÿæˆæ¨è
    add_to_history("refactor_function_tool", result)
    result["smart_recommendations"] = get_smart_recommendations("refactor_function_tool", result)
    
    return result


def generate_report(file_path: str, analysis_data: Dict[str, Any]) -> Dict[str, Any]:
    """
    ç”Ÿæˆç»¼åˆåˆ†ææŠ¥å‘Š
    
    Args:
        file_path: æ–‡ä»¶è·¯å¾„
        analysis_data: åˆ†ææ•°æ®
    """
    
    if "error" in analysis_data:
        result = {"error": "æ— æ•ˆçš„åˆ†ææ•°æ®"}
    else:
        total_functions = analysis_data.get("total_functions", 0)
        critical_functions = [
            node["name"] for node in analysis_data.get("dependency_nodes", [])
            if node.get("importance_score", 0) > 7
        ]
        
        modules = analysis_data.get("modules", {})
        
        result = {
            "file_path": file_path,
            "summary": {
                "total_functions": total_functions,
                "critical_functions": len(critical_functions),
                "modules": len(modules),
                "relationships": analysis_data.get("total_relationships", 0)
            },
            "key_findings": {
                "most_important_functions": critical_functions[:5],
                "module_distribution": {k: len(v) for k, v in modules.items()},
                "complexity_analysis": "åŸºäºä¾èµ–å…³ç³»åˆ†æçš„å¤æ‚åº¦è¯„ä¼°"
            },
            "recommendations": [
                "å…³æ³¨é«˜é‡è¦æ€§è¯„åˆ†çš„å‡½æ•°",
                "æ£€æŸ¥æ¨¡å—é—´çš„è€¦åˆåº¦",
                "è€ƒè™‘é‡æ„å¤æ‚åº¦è¿‡é«˜çš„å‡½æ•°"
            ]
        }
    
    # æ·»åŠ åˆ°å†å²è®°å½•å¹¶ç”Ÿæˆæ¨è
    add_to_history("generate_report_tool", result)
    result["smart_recommendations"] = get_smart_recommendations("generate_report_tool", result)
    
    return result


def search_code_tool(file_path: str, search_pattern: str, search_type: str = "function") -> Dict[str, Any]:
    """
    åœ¨ä»£ç ä¸­æœç´¢ç‰¹å®šæ¨¡å¼
    
    Args:
        file_path: ä»£ç æ–‡ä»¶è·¯å¾„
        search_pattern: æœç´¢æ¨¡å¼ï¼ˆå‡½æ•°åã€å˜é‡åã€å­—ç¬¦ä¸²ç­‰ï¼‰
        search_type: æœç´¢ç±»å‹ ("function", "variable", "string", "regex")
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        return {"error": f"æ— æ³•è¯»å–æ–‡ä»¶: {str(e)}"}
    
    lines = content.split('\n')
    results = []
    
    if search_type == "function":
        # æœç´¢å‡½æ•°å®šä¹‰å’Œè°ƒç”¨
        func_def_pattern = rf'\b{re.escape(search_pattern)}\s*\('
        func_call_pattern = rf'\b{re.escape(search_pattern)}\s*\('
        
        for i, line in enumerate(lines):
            if re.search(func_def_pattern, line):
                # åˆ¤æ–­æ˜¯å®šä¹‰è¿˜æ˜¯è°ƒç”¨
                is_definition = any(keyword in line for keyword in ['int ', 'void ', 'static ', 'char ', 'uint'])
                results.append({
                    "line_number": i + 1,
                    "line_content": line.strip(),
                    "match_type": "definition" if is_definition else "call",
                    "function_name": search_pattern
                })
    
    elif search_type == "variable":
        # æœç´¢å˜é‡ä½¿ç”¨
        var_pattern = rf'\b{re.escape(search_pattern)}\b'
        for i, line in enumerate(lines):
            if re.search(var_pattern, line):
                results.append({
                    "line_number": i + 1,
                    "line_content": line.strip(),
                    "match_type": "variable_usage"
                })
    
    elif search_type == "string":
        # æœç´¢å­—ç¬¦ä¸²å­—é¢é‡
        for i, line in enumerate(lines):
            if search_pattern in line:
                results.append({
                    "line_number": i + 1,
                    "line_content": line.strip(),
                    "match_type": "string_literal"
                })
    
    elif search_type == "regex":
        # æ­£åˆ™è¡¨è¾¾å¼æœç´¢
        try:
            pattern = re.compile(search_pattern)
            for i, line in enumerate(lines):
                if pattern.search(line):
                    results.append({
                        "line_number": i + 1,
                        "line_content": line.strip(),
                        "match_type": "regex_match"
                    })
        except re.error as e:
            return {"error": f"æ— æ•ˆçš„æ­£åˆ™è¡¨è¾¾å¼: {str(e)}"}
    
    result = {
        "search_pattern": search_pattern,
        "search_type": search_type,
        "total_matches": len(results),
        "results": results[:50],  # é™åˆ¶ç»“æœæ•°é‡
        "file_info": {
            "path": file_path,
            "total_lines": len(lines)
        }
    }
    
    # æ·»åŠ åˆ°å†å²è®°å½•å¹¶ç”Ÿæˆæ¨è
    add_to_history("search_code_tool", result)
    result["smart_recommendations"] = get_smart_recommendations("search_code_tool", result)
    
    return result


def analyze_data_structures_tool(file_path: str) -> Dict[str, Any]:
    """
    åˆ†æä»£ç ä¸­çš„æ•°æ®ç»“æ„
    
    Args:
        file_path: ä»£ç æ–‡ä»¶è·¯å¾„
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        return {"error": f"æ— æ³•è¯»å–æ–‡ä»¶: {str(e)}"}
    
    structures = []
    enums = []
    typedefs = []
    
    # æŸ¥æ‰¾ç»“æ„ä½“å®šä¹‰
    struct_pattern = r'typedef\s+struct\s*\{([^}]+)\}\s*(\w+);?|struct\s+(\w+)\s*\{([^}]+)\}'
    for match in re.finditer(struct_pattern, content, re.MULTILINE | re.DOTALL):
        if match.group(2):  # typedef struct
            struct_name = match.group(2)
            struct_body = match.group(1)
        else:  # regular struct
            struct_name = match.group(3)
            struct_body = match.group(4)
        
        # åˆ†æå­—æ®µ
        fields = []
        for line in struct_body.split('\n'):
            line = line.strip()
            if line and not line.startswith('//') and not line.startswith('/*'):
                # ç®€å•çš„å­—æ®µè§£æ
                field_match = re.search(r'(\w+(?:\s*\*)?)\s+(\w+)(?:\[.*?\])?\s*;', line)
                if field_match:
                    fields.append({
                        "type": field_match.group(1).strip(),
                        "name": field_match.group(2),
                        "line": line
                    })
        
        structures.append({
            "name": struct_name,
            "fields": fields,
            "field_count": len(fields),
            "estimated_size": len(fields) * 4  # ç²—ç•¥ä¼°è®¡
        })
    
    # æŸ¥æ‰¾æšä¸¾å®šä¹‰
    enum_pattern = r'typedef\s+enum\s*\{([^}]+)\}\s*(\w+);?|enum\s+(\w+)\s*\{([^}]+)\}'
    for match in re.finditer(enum_pattern, content, re.MULTILINE | re.DOTALL):
        if match.group(2):  # typedef enum
            enum_name = match.group(2)
            enum_body = match.group(1)
        else:  # regular enum
            enum_name = match.group(3)
            enum_body = match.group(4)
        
        # åˆ†ææšä¸¾å€¼
        values = []
        for line in enum_body.split('\n'):
            line = line.strip().rstrip(',')
            if line and not line.startswith('//'):
                values.append(line)
        
        enums.append({
            "name": enum_name,
            "values": values,
            "value_count": len(values)
        })
    
    # æŸ¥æ‰¾typedefå®šä¹‰
    typedef_pattern = r'typedef\s+([^;]+)\s+(\w+)\s*;'
    for match in re.finditer(typedef_pattern, content):
        base_type = match.group(1).strip()
        typedef_name = match.group(2)
        
        if not any(keyword in base_type for keyword in ['struct', 'enum']):
            typedefs.append({
                "name": typedef_name,
                "base_type": base_type
            })
    
    result = {
        "structures": structures,
        "enums": enums,
        "typedefs": typedefs,
        "summary": {
            "total_structures": len(structures),
            "total_enums": len(enums),
            "total_typedefs": len(typedefs),
            "complex_structures": [s for s in structures if s["field_count"] > 5]
        },
        "analysis": {
            "most_complex_struct": max(structures, key=lambda x: x["field_count"]) if structures else None,
            "data_model_complexity": "é«˜" if len(structures) > 10 else "ä¸­" if len(structures) > 3 else "ä½"
        }
    }
    
    # æ·»åŠ åˆ°å†å²è®°å½•å¹¶ç”Ÿæˆæ¨è
    add_to_history("analyze_data_structures_tool", result)
    result["smart_recommendations"] = get_smart_recommendations("analyze_data_structures_tool", result)
    
    return result


def security_audit_tool(file_path: str) -> Dict[str, Any]:
    """
    å®‰å…¨å®¡è®¡å·¥å…·ï¼Œæ£€æµ‹æ½œåœ¨çš„å®‰å…¨é—®é¢˜
    
    Args:
        file_path: ä»£ç æ–‡ä»¶è·¯å¾„
    """
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        return {"error": f"æ— æ³•è¯»å–æ–‡ä»¶: {str(e)}"}
    
    lines = content.split('\n')
    security_issues = []
    
    # å®šä¹‰å®‰å…¨æ£€æŸ¥è§„åˆ™
    security_rules = [
        {
            "pattern": r'\b(strcpy|strcat|sprintf|gets)\s*\(',
            "severity": "é«˜",
            "category": "ç¼“å†²åŒºæº¢å‡º",
            "description": "ä½¿ç”¨ä¸å®‰å…¨çš„å­—ç¬¦ä¸²å‡½æ•°ï¼Œå¯èƒ½å¯¼è‡´ç¼“å†²åŒºæº¢å‡º",
            "suggestion": "ä½¿ç”¨å®‰å…¨ç‰ˆæœ¬ï¼šstrncpy, strncat, snprintf, fgets"
        },
        {
            "pattern": r'\bmalloc\s*\([^)]+\)(?![^;]*free)',
            "severity": "ä¸­",
            "category": "å†…å­˜æ³„æ¼",
            "description": "mallocåˆ†é…çš„å†…å­˜å¯èƒ½æ²¡æœ‰å¯¹åº”çš„free",
            "suggestion": "ç¡®ä¿æ¯ä¸ªmallocéƒ½æœ‰å¯¹åº”çš„freeè°ƒç”¨"
        },
        {
            "pattern": r'\bsystem\s*\(',
            "severity": "é«˜",
            "category": "å‘½ä»¤æ³¨å…¥",
            "description": "ä½¿ç”¨system()å‡½æ•°å¯èƒ½å¯¼è‡´å‘½ä»¤æ³¨å…¥æ”»å‡»",
            "suggestion": "ä½¿ç”¨æ›´å®‰å…¨çš„APIå¦‚execv()ç³»åˆ—å‡½æ•°"
        },
        {
            "pattern": r'password|passwd|pwd',
            "severity": "ä¸­",
            "category": "æ•æ„Ÿä¿¡æ¯",
            "description": "å¯èƒ½åŒ…å«ç¡¬ç¼–ç çš„å¯†ç æˆ–æ•æ„Ÿä¿¡æ¯",
            "suggestion": "é¿å…åœ¨ä»£ç ä¸­ç¡¬ç¼–ç æ•æ„Ÿä¿¡æ¯"
        },
        {
            "pattern": r'\[\s*\w+\s*\]\s*=(?![^;]*sizeof)',
            "severity": "ä¸­",
            "category": "æ•°ç»„è¶Šç•Œ",
            "description": "æ•°ç»„è®¿é—®å¯èƒ½å­˜åœ¨è¶Šç•Œé£é™©",
            "suggestion": "æ·»åŠ è¾¹ç•Œæ£€æŸ¥æˆ–ä½¿ç”¨å®‰å…¨çš„æ•°ç»„æ“ä½œ"
        },
        {
            "pattern": r'rand\s*\(\)',
            "severity": "ä½",
            "category": "å¼±éšæœºæ•°",
            "description": "ä½¿ç”¨rand()ç”Ÿæˆçš„éšæœºæ•°ä¸å¤Ÿå®‰å…¨",
            "suggestion": "ä½¿ç”¨åŠ å¯†å®‰å…¨çš„éšæœºæ•°ç”Ÿæˆå™¨"
        }
    ]
    
    # æ£€æŸ¥æ¯ä¸€è¡Œ
    for i, line in enumerate(lines):
        for rule in security_rules:
            if re.search(rule["pattern"], line, re.IGNORECASE):
                security_issues.append({
                    "line_number": i + 1,
                    "line_content": line.strip(),
                    "severity": rule["severity"],
                    "category": rule["category"],
                    "description": rule["description"],
                    "suggestion": rule["suggestion"]
                })
    
    # ç»Ÿè®¡åˆ†æ
    severity_count = defaultdict(int)
    category_count = defaultdict(int)
    
    for issue in security_issues:
        severity_count[issue["severity"]] += 1
        category_count[issue["category"]] += 1
    
    # è®¡ç®—å®‰å…¨è¯„åˆ†
    high_weight = severity_count["é«˜"] * 3
    medium_weight = severity_count["ä¸­"] * 2
    low_weight = severity_count["ä½"] * 1
    total_weight = high_weight + medium_weight + low_weight
    
    security_score = max(0, 100 - total_weight * 2)
    
    result = {
        "security_issues": security_issues[:50],  # é™åˆ¶ç»“æœæ•°é‡
        "summary": {
            "total_issues": len(security_issues),
            "severity_distribution": dict(severity_count),
            "category_distribution": dict(category_count),
            "security_score": security_score
        },
        "risk_assessment": {
            "overall_risk": (
                "é«˜é£é™©" if security_score < 60 else
                "ä¸­é£é™©" if security_score < 80 else
                "ä½é£é™©"
            ),
            "critical_issues": [issue for issue in security_issues if issue["severity"] == "é«˜"],
            "recommendations": [
                "ä¼˜å…ˆä¿®å¤é«˜ä¸¥é‡æ€§é—®é¢˜",
                "å»ºç«‹ä»£ç å®‰å…¨å®¡æŸ¥æµç¨‹",
                "ä½¿ç”¨é™æ€åˆ†æå·¥å…·å®šæœŸæ£€æŸ¥",
                "åŸ¹è®­å¼€å‘äººå‘˜å®‰å…¨ç¼–ç¨‹å®è·µ"
            ]
        }
    }
    
    # æ·»åŠ åˆ°å†å²è®°å½•å¹¶ç”Ÿæˆæ¨è
    add_to_history("security_audit_tool", result)
    result["smart_recommendations"] = get_smart_recommendations("security_audit_tool", result)
    
    return result


def smart_assistant_tool(context: str = "") -> Dict[str, Any]:
    """
    æ™ºèƒ½åˆ†æåŠ©æ‰‹ï¼ŒåŸºäºå†å²è®°å½•æä¾›ä¸ªæ€§åŒ–å»ºè®®
    
    Args:
        context: ç”¨æˆ·æä¾›çš„ä¸Šä¸‹æ–‡ä¿¡æ¯
    """
    
    # åˆ†æå†å²è®°å½•
    recent_tools = [h["tool"] for h in analysis_history[-10:]]
    tool_frequency = defaultdict(int)
    for tool in recent_tools:
        tool_frequency[tool] += 1
    
    # ç”Ÿæˆå»ºè®®
    suggestions = []
    workflow_recommendations = []
    
    if not analysis_history:
        suggestions.extend([
            "ğŸš€ å¼€å§‹åˆ†æï¼šä½¿ç”¨ 'chunk_code_tool' å°†å¤§æ–‡ä»¶åˆ†å—å¤„ç†",
            "ğŸ” å¿«é€Ÿæœç´¢ï¼šä½¿ç”¨ 'search_code_tool' æŸ¥æ‰¾ç‰¹å®šå‡½æ•°æˆ–å…³é”®è¯",
            "ğŸ›¡ï¸ å®‰å…¨æ£€æŸ¥ï¼šä½¿ç”¨ 'security_audit_tool' å‘ç°æ½œåœ¨å®‰å…¨é—®é¢˜"
        ])
        workflow_recommendations.extend([
            "1. é¦–å…ˆä½¿ç”¨ä»£ç åˆ†å—å·¥å…·äº†è§£æ•´ä½“ç»“æ„",
            "2. ç„¶ååˆ†æå‡½æ•°ä¾èµ–å…³ç³»",
            "3. æ·±å…¥åˆ†æå…³é”®å‡½æ•°",
            "4. è¿›è¡Œå®‰å…¨å®¡è®¡",
            "5. ç”Ÿæˆç»¼åˆæŠ¥å‘Š"
        ])
    else:
        # åŸºäºå†å²è®°å½•çš„æ™ºèƒ½å»ºè®®
        last_tool = analysis_history[-1]["tool"]
        last_result = analysis_history[-1]["result"]
        
        if "chunk_code_tool" in recent_tools and "analyze_dependencies_tool" not in recent_tools:
            suggestions.append("ğŸ“Š å»ºè®®åˆ†æå‡½æ•°ä¾èµ–å…³ç³»ï¼Œäº†è§£ä»£ç ç»“æ„")
        
        if "security_audit_tool" not in recent_tools:
            suggestions.append("ğŸ›¡ï¸ å»ºè®®è¿›è¡Œå®‰å…¨å®¡è®¡ï¼Œå‘ç°æ½œåœ¨é—®é¢˜")
        
        if "analyze_data_structures_tool" not in recent_tools:
            suggestions.append("ğŸ—ï¸ å»ºè®®åˆ†ææ•°æ®ç»“æ„ï¼Œç†è§£æ•°æ®æ¨¡å‹")
        
        # æ£€æŸ¥æ˜¯å¦æœ‰é«˜å¤æ‚åº¦å‡½æ•°éœ€è¦é‡æ„
        for history in analysis_history[-5:]:
            if history["tool"] == "analyze_function_tool":
                complexity = history["result"].get("complexity_indicators", {}).get("line_count", 0)
                if complexity > 50:
                    suggestions.append(f"ğŸ”§ å‘ç°é«˜å¤æ‚åº¦å‡½æ•°ï¼Œå»ºè®®ä½¿ç”¨é‡æ„å·¥å…·ä¼˜åŒ–")
                    break
    
    # ç”Ÿæˆä¸ªæ€§åŒ–çš„ä¸‹ä¸€æ­¥å»ºè®®
    next_steps = []
    if context:
        context_lower = context.lower()
        if "å®‰å…¨" in context_lower or "security" in context_lower:
            next_steps.append("è¿›è¡Œå…¨é¢çš„å®‰å…¨å®¡è®¡")
        if "å‡½æ•°" in context_lower or "function" in context_lower:
            next_steps.append("åˆ†æç‰¹å®šå‡½æ•°çš„å®ç°ç»†èŠ‚")
        if "ç»“æ„" in context_lower or "struct" in context_lower:
            next_steps.append("åˆ†ææ•°æ®ç»“æ„å’Œç±»å‹å®šä¹‰")
        if "æœç´¢" in context_lower or "search" in context_lower:
            next_steps.append("ä½¿ç”¨ä»£ç æœç´¢åŠŸèƒ½å®šä½å…³é”®ä»£ç ")
    
    if not next_steps:
        next_steps = [
            "åˆ†æä»£ç æ•´ä½“ç»“æ„",
            "è¯†åˆ«å…³é”®å‡½æ•°å’Œä¾èµ–å…³ç³»",
            "æ£€æŸ¥æ½œåœ¨çš„å®‰å…¨é—®é¢˜",
            "ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"
        ]
    
    result = {
        "analysis_summary": {
            "total_analyses": len(analysis_history),
            "recent_tools_used": list(set(recent_tools)),
            "most_used_tool": max(tool_frequency.items(), key=lambda x: x[1])[0] if tool_frequency else None
        },
        "personalized_suggestions": suggestions,
        "workflow_recommendations": workflow_recommendations,
        "next_steps": next_steps,
        "tips": [
            "ğŸ’¡ ä½¿ç”¨å…·ä½“çš„å‡½æ•°åæˆ–å…³é”®è¯è¿›è¡Œæœç´¢æ›´æœ‰æ•ˆ",
            "ğŸ¯ å…³æ³¨é«˜é‡è¦æ€§è¯„åˆ†çš„å‡½æ•°ï¼Œå®ƒä»¬é€šå¸¸æ˜¯ç³»ç»Ÿæ ¸å¿ƒ",
            "ğŸ”„ å®šæœŸè¿›è¡Œå®‰å…¨å®¡è®¡ï¼Œä¿æŒä»£ç è´¨é‡",
            "ğŸ“ˆ ä½¿ç”¨ä¾èµ–å›¾å¯è§†åŒ–å¤æ‚çš„è°ƒç”¨å…³ç³»",
            "ğŸ› ï¸ å¯¹å¤æ‚å‡½æ•°è¿›è¡Œé‡æ„ï¼Œæé«˜ä»£ç å¯ç»´æŠ¤æ€§"
        ],
        "context_analysis": f"åŸºäºä¸Šä¸‹æ–‡ '{context}'ï¼Œä¸ºæ‚¨å®šåˆ¶äº†ä¸“é—¨çš„åˆ†æå»ºè®®" if context else "é€šç”¨åˆ†æå»ºè®®",
        "suggested_commands": [
            "æœç´¢ modbus",
            "åˆ†æå‡½æ•° main",
            "å®‰å…¨å®¡è®¡",
            "åˆ†ææ•°æ®ç»“æ„",
            "ç”Ÿæˆä¾èµ–å›¾"
        ]
    }
    
    # æ·»åŠ åˆ°å†å²è®°å½•
    add_to_history("smart_assistant_tool", result)
    
    return result 